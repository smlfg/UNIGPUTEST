# QLoRA Fine-Tuning Configuration
# Optimized for NVIDIA L40S (48GB VRAM)

model:
  # Model selection (HuggingFace model ID)
  name: "meta-llama/Llama-3.2-3B-Instruct"  # Fast, efficient, ~6GB in fp16
  # Alternative: "meta-llama/Llama-3.1-8B-Instruct" (requires more VRAM)

  # Quantization settings (for QLoRA)
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

# LoRA adapter configuration
lora:
  r: 16  # LoRA rank (higher = more parameters, better quality)
  lora_alpha: 32  # LoRA scaling factor
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"
  task_type: "CAUSAL_LM"

# Training hyperparameters
training:
  output_dir: "./checkpoints"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4  # Effective batch size = 16
  gradient_checkpointing: true

  # Optimizer settings
  optim: "paged_adamw_32bit"
  learning_rate: 2.0e-4
  weight_decay: 0.01
  max_grad_norm: 0.3
  warmup_ratio: 0.03

  # Learning rate schedule
  lr_scheduler_type: "cosine"

  # Logging & checkpointing
  logging_steps: 10
  save_steps: 100
  eval_steps: 100
  save_total_limit: 3

  # Mixed precision training
  fp16: false
  bf16: true  # Better for modern GPUs

  # Other settings
  remove_unused_columns: false
  group_by_length: true
  report_to: "tensorboard"  # or "wandb"

# Dataset configuration
dataset:
  # HuggingFace dataset or local path
  name: "iamtarun/python_code_instructions_18k_alpaca"  # Code instruction dataset
  # Alternative datasets:
  # - "HuggingFaceH4/code_instructions_120k" (larger)
  # - "teknium/GPT4-LLM-Cleaned" (general chat)
  # - Custom local dataset

  split:
    train: 0.95
    eval: 0.05

  max_seq_length: 2048  # Maximum sequence length
  packing: false  # Pack multiple samples into one sequence (advanced)

# Prompt template (Alpaca format)
prompt:
  template: |
    Below is an instruction that describes a task. Write a response that appropriately completes the request.

    ### Instruction:
    {instruction}

    ### Response:
    {output}

  input_template: |
    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

    ### Instruction:
    {instruction}

    ### Input:
    {input}

    ### Response:
    {output}

# Hardware & performance
hardware:
  device: "cuda"  # or "cpu"
  max_memory:
    0: "46GB"  # Leave 2GB for system overhead
  torch_dtype: "bfloat16"

# Evaluation settings
evaluation:
  do_eval: true
  evaluation_strategy: "steps"
  eval_accumulation_steps: 1

# Reproducibility
seed: 42
