{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ LLM Fine-Tuning Quick Start Demo\n",
    "\n",
    "This notebook demonstrates the complete pipeline:\n",
    "1. GPU availability check\n",
    "2. Model loading (PyTorch)\n",
    "3. Inference testing\n",
    "4. Fine-tuning setup\n",
    "5. ONNX export\n",
    "6. Benchmarking\n",
    "\n",
    "**Hardware**: NVIDIA L40S (48GB VRAM)  \n",
    "**Target**: Cross-platform deployment (GPU ‚Üí ONNX ‚Üí NPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "print(\"‚úÖ Path configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Availability Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.gpu_check import check_gpu_availability, print_gpu_info\n",
    "\n",
    "gpu_info = check_gpu_availability()\n",
    "print_gpu_info(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Model (Demo)\n",
    "\n",
    "We'll use a small model for quick testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Use a small model for demo\n",
    "model_name = \"gpt2\"  # Change to \"meta-llama/Llama-3.2-3B-Instruct\" for full model\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=50):\n",
    "    \"\"\"Generate text from prompt\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test generation\n",
    "prompt = \"Write a Python function to calculate fibonacci numbers:\"\n",
    "result = generate_text(prompt)\n",
    "\n",
    "print(\"üìù Generated text:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.config_loader import ConfigLoader\n",
    "\n",
    "# Load training config\n",
    "config_loader = ConfigLoader()\n",
    "config = config_loader.load()\n",
    "\n",
    "print(\"üìã Training Configuration:\")\n",
    "print(f\"  Model: {config['model']['name']}\")\n",
    "print(f\"  Dataset: {config['dataset']['name']}\")\n",
    "print(f\"  Batch Size: {config['training']['per_device_train_batch_size']}\")\n",
    "print(f\"  Learning Rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  LoRA Rank: {config['lora']['r']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset Loading Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.dataset import DatasetLoader\n",
    "\n",
    "# Load dataset (small subset for demo)\n",
    "dataset_loader = DatasetLoader(\n",
    "    dataset_name=\"iamtarun/python_code_instructions_18k_alpaca\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512,  # Smaller for demo\n",
    "    train_split=0.99,  # Use most for training\n",
    "    eval_split=0.01,\n",
    ")\n",
    "\n",
    "datasets = dataset_loader.load()\n",
    "\n",
    "print(f\"üìä Dataset Statistics:\")\n",
    "print(f\"  Train samples: {len(datasets['train'])}\")\n",
    "print(f\"  Eval samples: {len(datasets['eval'])}\")\n",
    "print(f\"\\nüìù Sample data:\")\n",
    "print(datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.model_utils import print_model_memory\n",
    "\n",
    "print_model_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Start Training (Commented Out)\n",
    "\n",
    "Uncomment to start training. This will take 2-3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.training.train import main\n",
    "\n",
    "# # Start training\n",
    "# main(config_path=\"../configs/training_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training completes, load the fine-tuned model\n",
    "\n",
    "# fine_tuned_model_path = \"../checkpoints/final_model\"\n",
    "\n",
    "# tokenizer_ft = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "# model_ft = AutoModelForCausalLM.from_pretrained(\n",
    "#     fine_tuned_model_path,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# print(\"‚úÖ Fine-tuned model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Before/After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prompt = \"Write a Python function to reverse a string:\"\n",
    "\n",
    "# print(\"üîµ Base Model:\")\n",
    "# print(generate_text(test_prompt, model, tokenizer))\n",
    "# print()\n",
    "\n",
    "# print(\"üü¢ Fine-Tuned Model:\")\n",
    "# print(generate_text(test_prompt, model_ft, tokenizer_ft))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Benchmark Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.evaluation.benchmark import quick_benchmark\n",
    "\n",
    "# # Benchmark fine-tuned model\n",
    "# result = quick_benchmark(\n",
    "#     model_path=\"../checkpoints/final_model\",\n",
    "#     device=\"cuda\",\n",
    "#     num_runs=50,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.export.onnx_export import export_model_to_onnx\n",
    "\n",
    "# # Export to ONNX with optimization and quantization\n",
    "# export_model_to_onnx(\n",
    "#     model_path=\"../checkpoints/final_model\",\n",
    "#     output_path=\"../models/onnx_model\",\n",
    "#     optimize=True,\n",
    "#     quantize=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary & Next Steps\n",
    "\n",
    "You've completed the demo! üéâ\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run full training with larger model (Llama 3.2 3B)\n",
    "2. Export to ONNX for deployment\n",
    "3. Benchmark GPU vs NPU performance\n",
    "4. Deploy on Snapdragon X Elite\n",
    "\n",
    "**Resources:**\n",
    "- Training script: `python src/training/train.py`\n",
    "- Config: `configs/training_config.yaml`\n",
    "- Documentation: `README.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
