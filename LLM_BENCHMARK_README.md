# Advanced LLM Benchmark Suite for NVIDIA L40S

Umfassendes Benchmark-System fÃ¼r Large Language Models mit erweiterten Metriken, Monitoring und automatischer Berichterstellung.

## ğŸš€ Features

### 1. **UnterstÃ¼tzte Modelle**
- âœ… **Falcon**: 7B, 13B
- âœ… **Mistral**: 7B
- âœ… **Llama 2**: 7B, 13B
- âœ… **CodeLlama**: 7B, 34B
- âœ… **GPT-2**: Small, Medium, Large, XL

### 2. **Quantisierungen**
- **FP32**: Full Precision (32-bit)
- **FP16**: Half Precision (16-bit)
- **BF16**: Brain Float 16
- **INT8**: 8-bit Quantisierung (bitsandbytes)
- **INT4**: 4-bit Quantisierung (NF4)
- **Dynamic**: PyTorch Dynamic Quantization

### 3. **Erweiterte Metriken**
- âš¡ **Performance**: Tokens/s, Latency, Throughput
- ğŸ’¾ **Memory**: Allocated, Reserved, Peak, Fragmentation
- ğŸŒ¡ï¸ **Thermal**: GPU Temperature (Avg/Peak)
- âš¡ **Power**: Power Consumption in Watt
- ğŸ“Š **GPU**: Utilization, Clock Speeds
- ğŸ”„ **Cache**: Cache Hit Rates

### 4. **Batch Processing**
- Automatisches Testing verschiedener Batch-GrÃ¶ÃŸen (1, 2, 4, 8, 16, 32)
- Scaling-Effizienz-Analyse
- Durchsatz-Optimierung

### 5. **Automatisierte Reports**
- ğŸ“„ **JSON**: Strukturierte Daten fÃ¼r weitere Verarbeitung
- ğŸ“Š **CSV**: Excel-kompatibel fÃ¼r Analysen
- ğŸ“ **Markdown**: Lesbare Berichte mit Tabellen
- ğŸ“ˆ **Visualisierungen**: Plots und Diagramme
- ğŸ”„ **Session-Vergleiche**: Trend-Analysen Ã¼ber Zeit

## ğŸ“¦ Installation

### Voraussetzungen
- NVIDIA L40S GPU (48GB VRAM)
- CUDA 11.8 oder hÃ¶her
- Python 3.8+

### Setup

```bash
# Repository klonen
git clone <your-repo-url>
cd UNIGPUTEST

# Virtual Environment erstellen
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# oder
venv\Scripts\activate  # Windows

# Dependencies installieren
pip install -r requirements.txt

# PyTorch mit CUDA (falls nicht in requirements.txt)
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Optional: HuggingFace Login fÃ¼r gated models (Llama 2)
huggingface-cli login
```

## ğŸ¯ Verwendung

### Quick Start

```bash
# Schneller Test mit GPT-2
python llm_benchmark.py --quick

# Einzelnes Modell benchmarken
python llm_benchmark.py --model gpt2 --quantization fp16 --batch-sizes 1 4 8

# Mistral 7B mit verschiedenen Quantisierungen
python llm_benchmark.py --model mistralai/Mistral-7B-v0.1 \
    --quantization int8 --batch-sizes 1 2 4 8 16

# Llama 2 13B mit INT4 Quantisierung
python llm_benchmark.py --model meta-llama/Llama-2-13b-hf \
    --quantization int4 --batch-sizes 1 2 4
```

### VollstÃ¤ndige Test-Suite

```bash
# Alle Modelle mit allen Quantisierungen (dauert mehrere Stunden!)
python llm_benchmark.py --config model_config.yaml

# Nur 7B Modelle
python llm_benchmark.py \
    --model mistralai/Mistral-7B-v0.1 \
    --model meta-llama/Llama-2-7b-hf \
    --model codellama/CodeLlama-7b-hf \
    --quantization fp16 int8
```

### GPU Monitoring

```bash
# Echtzeit GPU Monitoring mit curses UI
python gpu_monitor.py

# Einfacher Output (fÃ¼r Logging)
python gpu_monitor.py --simple

# Monitoring fÃ¼r 300 Sekunden
python gpu_monitor.py --duration 300

# Schnelleres Update (10x pro Sekunde)
python gpu_monitor.py --interval 0.1
```

### Report Generierung

```bash
# Alle Reports generieren
python report_generator.py --results-dir benchmark_results --plots

# Nur Markdown Report
python report_generator.py --format markdown

# Session Vergleich
python report_generator.py --compare-sessions \
    benchmark_results_20240101 \
    benchmark_results_20240115
```

## ğŸ“‹ Konfiguration

### model_config.yaml

Die Datei `model_config.yaml` enthÃ¤lt:
- Modell-Definitionen mit erwarteten Memory-Anforderungen
- Quantisierungs-Konfigurationen
- Benchmark-Presets (quick, standard, comprehensive)
- GPU-spezifische Einstellungen
- HuggingFace Hub Settings

```yaml
# Beispiel: Custom Benchmark
benchmark:
  my_test:
    models:
      - "mistralai/Mistral-7B-v0.1"
      - "meta-llama/Llama-2-7b-hf"
    quantizations: ["fp16", "int8"]
    batch_sizes: [1, 4, 8]
    iterations: 10
    warmup: 3
```

## ğŸ“Š Output-Struktur

```
benchmark_results/
â”œâ”€â”€ gpt2_fp16_20240107_143022.json
â”œâ”€â”€ mistral-7b_int8_20240107_144533.json
â””â”€â”€ ...

reports/
â”œâ”€â”€ report_20240107_150000.json
â”œâ”€â”€ report_20240107_150000.csv
â”œâ”€â”€ report_20240107_150000.md
â”œâ”€â”€ viz_20240107_150000_throughput.png
â”œâ”€â”€ viz_20240107_150000_memory.png
â””â”€â”€ viz_20240107_150000_power_efficiency.png
```

## ğŸ”§ Erweiterte Features

### 1. Memory Fragmentation Tracking

Das System trackt automatisch GPU Memory Fragmentation:
```python
fragmentation = (reserved_memory - allocated_memory) / reserved_memory
```

### 2. Power Efficiency Analysis

Berechnet Tokens pro Watt fÃ¼r jede Konfiguration:
```python
efficiency = tokens_per_second / power_consumption_watts
```

### 3. Batch Size Scaling

Misst wie gut ein Modell mit grÃ¶ÃŸeren Batches skaliert:
```python
scaling_efficiency = actual_throughput / expected_linear_throughput
```

### 4. Cache Hit Rates

Analysiert KV-Cache Effizienz (wenn verfÃ¼gbar).

## ğŸ“ˆ Benchmark-Ergebnisse Interpretieren

### Markdown Report Sections

1. **Executive Summary**: Ãœberblick Ã¼ber getestete Modelle
2. **Performance Highlights**: Best-of-Kategorie Winners
3. **Detailed Results**: VollstÃ¤ndige Tabellen pro Modell
4. **Batch Scaling Analysis**: Skalierungs-Effizienz
5. **Memory Analysis**: Speicherverbrauch Details
6. **Power & Thermal**: Energie-Effizienz
7. **Recommendations**: Automatische Empfehlungen

### Wichtige Metriken

| Metrik | Beschreibung | Ziel |
|--------|--------------|------|
| **Throughput** | Tokens/Sekunde | HÃ¶her ist besser |
| **Latency** | Zeit pro Iteration (ms) | Niedriger ist besser |
| **Memory Peak** | Maximaler VRAM-Verbrauch | Unter 45GB fÃ¼r L40S |
| **Fragmentation** | Verschwendeter Speicher | < 10% optimal |
| **Power Efficiency** | Tokens/s/Watt | HÃ¶her ist besser |
| **Scaling Efficiency** | Batch-Skalierung | > 90% gut |

## ğŸ“ Best Practices

### FÃ¼r L40S (48GB VRAM)

1. **7B Modelle**:
   - FP16: Batch 8-16 optimal
   - INT8: Batch 16-32 mÃ¶glich
   - INT4: Batch 32+ mÃ¶glich

2. **13B Modelle**:
   - FP16: Nur kleine Batches (1-4)
   - INT8: Batch 8-16 empfohlen âœ…
   - INT4: Batch 16-32 optimal

3. **34B Modelle**:
   - FP16: Passt nicht
   - INT8: MÃ¶glich mit Batch 1-4
   - INT4: Batch 4-8 empfohlen âœ…

### Quantisierungs-Empfehlungen

- **Entwicklung/Debugging**: FP16 (beste Accuracy)
- **Production Serving**: INT8 (guter Kompromiss)
- **Maximum Throughput**: INT4 (akzeptable Quality)
- **Research**: BF16 (numerische StabilitÃ¤t)

## ğŸ”¬ Beispiel-Workflows

### Workflow 1: Model Selection

```bash
# 1. Quick Test mehrerer Modelle
python llm_benchmark.py --quick

# 2. Detailed Test der Top 3
for model in mistral llama2-7b codellama-7b; do
    python llm_benchmark.py --model $model --quantization fp16 int8
done

# 3. Report generieren und vergleichen
python report_generator.py --plots
```

### Workflow 2: Optimization

```bash
# 1. Baseline mit FP16
python llm_benchmark.py --model mistralai/Mistral-7B-v0.1 \
    --quantization fp16 --batch-sizes 1 2 4 8 16

# 2. INT8 Quantisierung
python llm_benchmark.py --model mistralai/Mistral-7B-v0.1 \
    --quantization int8 --batch-sizes 1 2 4 8 16 32

# 3. Vergleich
python report_generator.py --format markdown
```

### Workflow 3: Production Planning

```bash
# 1. Test mit realistischen Batch Sizes
python llm_benchmark.py --model meta-llama/Llama-2-13b-hf \
    --quantization int8 --batch-sizes 4 8 16

# 2. Monitor wÃ¤hrend Benchmark
# Terminal 1:
python llm_benchmark.py ...

# Terminal 2:
python gpu_monitor.py

# 3. Analyse
python report_generator.py --plots
```

## ğŸ› Troubleshooting

### Out of Memory Errors

```bash
# LÃ¶sung 1: Kleinere Batch Size
python llm_benchmark.py --model ... --batch-sizes 1 2

# LÃ¶sung 2: StÃ¤rkere Quantisierung
python llm_benchmark.py --model ... --quantization int4

# LÃ¶sung 3: GPU Cache leeren
python -c "import torch; torch.cuda.empty_cache()"
```

### Model Download Fehler

```bash
# HuggingFace Token setzen
export HF_TOKEN="your_token_here"

# Oder Login
huggingface-cli login

# Cache Verzeichnis setzen
export TRANSFORMERS_CACHE="./models_cache"
```

### CUDA Errors

```bash
# CUDA Version prÃ¼fen
nvidia-smi

# PyTorch CUDA Support prÃ¼fen
python -c "import torch; print(torch.cuda.is_available())"

# Neuinstallation mit korrekter CUDA Version
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

## ğŸ“š Weitere Ressourcen

- [HuggingFace Model Hub](https://huggingface.co/models)
- [bitsandbytes Documentation](https://github.com/TimDettmers/bitsandbytes)
- [NVIDIA L40S Specs](https://www.nvidia.com/en-us/data-center/l40s/)
- [Transformers Documentation](https://huggingface.co/docs/transformers)

## ğŸ¤ Contributing

Contributions sind willkommen! Bitte:
1. Fork das Repository
2. Erstelle einen Feature Branch
3. Teste deine Ã„nderungen
4. Submit einen Pull Request

## ğŸ“„ Lizenz

[Ihre Lizenz hier]

## ğŸ™ Credits

Entwickelt fÃ¼r NVIDIA L40S GPU Performance Testing

---

**Letzte Aktualisierung**: 2025-01-07
**Version**: 1.0.0
